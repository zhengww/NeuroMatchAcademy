{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": [
          "# Tutorial 1- Optimal Control for Discrete State\n",
          "\n",
          "Please execute the cell below to initialize the notebook environment.\n",
          "\n",
          "import numpy as np                 # import numpy\n",
          "import scipy               # import scipy\n",
          "import random                      # import basic random number generator functions\n",
          "from scipy.linalg import inv\n",
          "\n",
          "import matplotlib.pyplot as plt    # import matplotlib\n",
          "\n",
          "---\n",
          "\n",
          "## Tutorial objectives\n",
          "\n",
          "In this tutorial, we will implement a binary HMM task.\n",
          "\n",
          "---\n",
          "\n",
          "## Task Description\n",
          "\n",
          "There are two boxes. The box can be in a high-rewarding state ($s=1$), which means that a reward will be delivered with high probabilty $q_{high}$; or the box can be in low-rewarding state ($s=0$), then the reward will be delivered with low probabilty $q_{low}$.\n",
          "\n",
          "The states of the two boxes are latent. At a certain time, only one of the sites can be in high-rewarding state, and the other box will be the opposite. The states of the two boxes switches with a certain probability $p_{sw}$. \n",
          "\n",
          "![alt text](switching.png \"Title\")\n",
          "\n",
          "\n",
          "The agent may stay at one site for sometime. As the agent accumulates evidence about the state of the box on that site, it may choose to stay or switch to the other side with a switching cost $c$. The agent keeps beliefs on the states of the boxes, which is the posterior probability of the state being high-rewarding given all the past observations. Consider the belief on the state of the left box, we have \n",
          "\n",
          "$$b(s_t) = p(s_t = 1 | o_{0:t}, l_{0:t}, a_{0:t-1})$$\n",
          "\n",
          "where $o$ is the observation that whether a reward is obtained, $l$ is the location of the agent, $a$ is the action of staying ($a=0$) or switching($a=1$). \n",
          "\n",
          "Since the two boxes are completely anti-correlated, i.e. only one of the boxes is high-rewarded at a certain time, the the other one is low-rewarded, the belief on the two boxes should sum up to be 1. As a result, we only need to track the belief on one of the boxes. \n",
          "\n",
          "The policy of the agent depends on a threshold on beliefs. When the belief on the box on the other side gets higher than the threshold $\\theta$, the agent will switch to the other side. In other words, the agent will choose to switch when it is confident enough that the other side is high rewarding. \n",
          "\n",
          "The value function can be defined as the reward rate during a single trial.\n",
          "\n",
          "$$v(\\theta) = \\sum_t r_t - c\\cdot 1_{a_t = 1}$$ \n",
          "\n",
          "we would like to see the relation between the threshold and the value function. \n",
          "\n",
          "### Exercise 1: Control for binary HMM\n",
          "In this excercise, we generate the dynamics for the binary HMM task as described above. \n",
          "\n",
          "# This function is the policy based on threshold\n",
          "\n",
          "def policy(threshold, bel, loc):\n",
          "    if loc == 0:\n",
          "        if bel[1]  >= threshold:\n",
          "            act = 1\n",
          "        else:\n",
          "            act = 0\n",
          "    else:  # loc = 1\n",
          "        if bel[0] >= threshold:\n",
          "            act = 1\n",
          "        else:\n",
          "            act = 0\n",
          "\n",
          "    return act\n",
          "\n",
          "# This function generates the dynamics\n",
          "\n",
          "def generateProcess(params):\n",
          "\n",
          "    T, p_sw, q_high, q_low, cost_sw, threshold = params\n",
          "    world_state = np.zeros((2, T), int)  # value :1: good box; 0: bad box\n",
          "    loc = np.zeros(T, int)  # 0: left box               1: right box\n",
          "    obs = np.zeros(T, int)  # 0: did not get food        1: get food\n",
          "    act = np.zeros(T, int)  # 0 : stay                   1: switch and get food from the other side\n",
          "    bel = np.zeros((2, T), float)  # the probability that the left box has food,\n",
          "    # then the probability that the second box has food is 1-b\n",
          "\n",
          "\n",
          "    p = np.array([1 - p_sw, p_sw])  # transition probability to good state\n",
          "    q = np.array([q_low, q_high])\n",
          "    q_mat = np.array([[1 - q_high, q_high], [1 - q_low, q_low]])\n",
          "\n",
          "    for t in range(T):\n",
          "        if t == 0:\n",
          "            world_state[0, t] = 1    # good box\n",
          "            world_state[1, t] = 1 - world_state[0, t]\n",
          "            loc[t] = 0\n",
          "            obs[t] = 0\n",
          "            bel_0 = np.random.random(1)[0]\n",
          "            bel[:, t] = np.array([bel_0, 1-bel_0])\n",
          "\n",
          "            act[t] = policy(threshold, bel[:, t], loc[t])\n",
          "\n",
          "        else:\n",
          "            world_state[0, t] = np.random.binomial(1, p[world_state[0, t - 1]])\n",
          "            world_state[1, t] = 1 - world_state[0, t]\n",
          "\n",
          "            if act[t - 1] == 0:\n",
          "                loc[t] = loc[t - 1]\n",
          "            else:  # after weitching, open the new box, deplete if any; then wait a usualy time\n",
          "                loc[t] = 1 - loc[t - 1]\n",
          "\n",
          "            # new observation\n",
          "            obs[t] = np.random.binomial(1, q[world_state[loc[t], t-1]])\n",
          "\n",
          "            # update belief posterior, p(s[t] | obs(0-t), act(0-t-1))\n",
          "            bel_0 = (bel[0, t-1] * p_sw  + bel[1, t-1] * (1 - p_sw)) * q_mat[loc[t], obs[t]]\n",
          "            bel_1 = (bel[1, t - 1] * p_sw + bel[0, t - 1] * (1 - p_sw)) * q_mat[1-loc[t], obs[t]]\n",
          "\n",
          "            bel[0, t] = bel_0 / (bel_0 + bel_1)\n",
          "            bel[1, t] = bel_1 / (bel_0 + bel_1)\n",
          "\n",
          "            act[t] = policy(threshold, bel[:, t], loc[t])\n",
          "\n",
          "    return bel, obs, act, world_state, loc\n",
          "\n",
          "# value function \n",
          "def value_function(obs, act, cost_sw, discount):\n",
          "    T = len(obs)\n",
          "    discount_time = np.array([discount ** t for t in range(T)])\n",
          "\n",
          "    #value = (np.sum(obs) - np.sum(act) * cost_sw) / T\n",
          "    value = (np.sum(np.multiply(obs, discount_time)) - np.sum(np.multiply(act, discount_time)) * cost_sw) / T\n",
          "\n",
          "    return value\n",
          "\n",
          "def switch_int(obs, act):\n",
          "    sw_t = np.where(act == 1)[0]\n",
          "    sw_int = sw_t[1:] - sw_t[:-1]\n",
          "\n",
          "    return sw_int\n",
          "\n",
          "#Plotting \n",
          "def plot_dynamics(bel, obs, act, world_state, loc):\n",
          "    T = len(obs)\n",
          "\n",
          "    showlen = min(T, 100)\n",
          "    startT = 0\n",
          "\n",
          "    endT = startT + showlen\n",
          "    showT = range(startT, endT)\n",
          "    time_range = np.linspace(0, showlen - 1)\n",
          "\n",
          "    fig_posterior, [ax0, ax1, ax_loc, ax2, ax3] = plt.subplots(5, 1, figsize=(15, 10))\n",
          "\n",
          "    ax0.plot(world_state[0, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax0.set_ylabel('Left box', rotation=360, fontsize=22)\n",
          "    ax0.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax0.set_xticks(np.arange(0, showlen, 10))\n",
          "    ax0.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax0.set_xlim([0, showlen])\n",
          "\n",
          "\n",
          "    ax3.plot(world_state[1, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax3.set_ylabel('Right box', rotation=360, fontsize=22)\n",
          "    ax3.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax3.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax3.set_xlim([0, showlen])\n",
          "    ax3.set_xticks(np.arange(0, showlen, 10))\n",
          "\n",
          "    ax1.plot(bel[0, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax1.plot(time_range, threshold * np.ones(time_range.shape), 'r--')\n",
          "    ax1.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax1.set_ylabel('Belief on \\n left box', rotation=360, fontsize=22)\n",
          "    ax1.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax1.set_xlim([0, showlen])\n",
          "    ax1.set_ylim([0, 1])\n",
          "    ax1.set_xticks(np.arange(0, showlen, 10))\n",
          "\n",
          "\n",
          "    ax_loc.plot(1 - loc[showT], 'g.-', markersize=12, linewidth=5, label = 'location')\n",
          "    ax_loc.plot((act[showT] - .1) * .8, 'v', markersize=10, label = 'action')\n",
          "    ax_loc.plot(obs[showT] * .5, '*', markersize=5, label = 'reward')\n",
          "    ax_loc.legend(loc=\"upper right\")\n",
          "    ax_loc.set_xlim([0, showlen])\n",
          "    ax_loc.set_ylim([0, 1])\n",
          "    #ax_loc.set_yticks([])\n",
          "    ax_loc.set_xticks([0, showlen])\n",
          "    ax_loc.tick_params(axis='both', which='major', labelsize=18)\n",
          "    labels = [item.get_text() for item in ax_loc.get_yticklabels()]\n",
          "    labels[0] = 'Right'\n",
          "    labels[-1] = 'Left'\n",
          "    ax_loc.set_yticklabels(labels)\n",
          "\n",
          "    ax2.plot(bel[1, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax2.plot(time_range, threshold * np.ones(time_range.shape), 'r--')\n",
          "    ax2.set_xlabel('time', fontsize=18)\n",
          "    ax2.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax2.set_ylabel('Belief on  \\n  right box', rotation=360, fontsize=22)\n",
          "    ax2.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax2.set_xlim([0, showlen])\n",
          "    ax2.set_ylim([0, 1])\n",
          "    ax2.set_xticks(np.arange(0, showlen, 10))\n",
          "\n",
          "    plt.show()\n",
          "\n",
          "def plot_val_thre(threshold_array, value_array):\n",
          "    fig_, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
          "    ax.plot(threshold_array, value_array)\n",
          "    ax.set_ylim([np.min(value_array), np.max(value_array)])\n",
          "    ax.set_title('threshold vs value')\n",
          "    ax.set_xlabel('threshold')\n",
          "    ax.set_ylabel('value')\n",
          "    plt.show()\n",
          "\n",
          "T = 5000\n",
          "p_sw = .95          # state transiton probability\n",
          "q_high = .7\n",
          "q_low = 0 #.2\n",
          "cost_sw = 1 #int(1/(1-p_sw)) - 5\n",
          "threshold = .8    # threshold of belief for switching\n",
          "discount = 1\n",
          "\n",
          "step = 0.1\n",
          "threshold_array = np.arange(0, 1 + step, step)\n",
          "value_array = np.zeros(threshold_array.shape)\n",
          "\n",
          "for i in range(len(threshold_array)):\n",
          "    threshold = threshold_array[i]\n",
          "    params = [T, p_sw, q_high, q_low, cost_sw, threshold]\n",
          "    bel, obs, act, world_state, loc = generateProcess(params)\n",
          "    value_array[i] = value_function(obs, act, cost_sw, discount)\n",
          "    sw_int = switch_int(obs, act)\n",
          "    #print(np.mean(sw_int))\n",
          "\n",
          "    if threshold == 0.8:\n",
          "        plot_dynamics(bel, obs, act, world_state, loc)\n",
          "\n",
          "plot_val_thre(threshold_array, value_array)\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n"
        ]
      }
    },
    "colab": {
      "name": "Copy of Tutorial1_optimal_control_discrete (no comment).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdNoX-pa3k7x",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial 1- Optimal Control and Fishing\n",
        "\n",
        "(By Zhengwei Wu and Xaq Pitkow)\n",
        "\n",
        "Please execute the cell below to initialize the notebook environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T2eacv13k7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np                 # import numpy\n",
        "import scipy                       # import scipy\n",
        "import random                      # import basic random number generator functions\n",
        "from scipy.linalg import inv\n",
        "\n",
        "import matplotlib.pyplot as plt    # import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "fig_w, fig_h = (8, 6)\n",
        "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})\n",
        "#plt.style.use('dark_background')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYqoaTCk3k76",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## Tutorial objectives\n",
        "\n",
        "In this tutorial, we will implement a binary control task: a Partially Observable Markov Decision Process (POMDP) that describes fishing. The agent (you) seeks reward from two fishing sites without directly observing where the school of fish is. Based on when and where you catch fish, you keep updating your belief about the fish location, i.e. the posterior of the fish given past observations. You should control your position to get the most fish while minimizing the cost of switching sides.\n",
        "\n",
        "You've already learned about stochastic dynamics, latent states, and measurements. Now we introduce you to the new concepts of **control, utility, policy**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KKRwWuo3k78",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## Task Description\n",
        "\n",
        "There are two locations for the fish and you (Left and Right). If you're on the same side as the fish, you'll catch more, with probabilty $q_{\\rm high}$ per discrete time step. Otherwise you may still catch fish with probability $q_{\\rm low}$. One fish is worth 1 \"point\".\n",
        "\n",
        "The fish location is latent. The only information you have about the fish location is when you catch one. Secretly the fish switch sides with a certain probability $p_{\\rm sw} = 1 - p_{\\rm stay}$.\n",
        "\n",
        "\n",
        "You are in control of your location. You may stay on your current side with no cost, or switch to the other side and incur an action cost $c$ (again, in units of fish).\n",
        "\n",
        "You select controls or actions by following a **policy**. This defines what to do in any situation. Here the situation is specified by your location and your belief about the fish location. For optimal control we assume that this belief is the posterior probability over the current fish location, given all the past measurements. We only need one number for this, since the fish are either on the left or the right. So we write \n",
        "\n",
        "$$b_t = p(s^{\\rm fish}_t = {\\rm Right}\\  |\\  m_{0:t}, a_{0:t-1})$$\n",
        "\n",
        "where $m$ are the measurements, and $a$ are the controls or actions (stay or switch).\n",
        "\n",
        "Here we parameterize the policy by a simple threshold on beliefs. (This happens to be optimal if you pick the right threshold!) When your belief that fish are on the current side falls below a threshold $\\theta$, you switch to the other side.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E6RfR5w3k79",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Exercise 1: Binary HMM task\n",
        "In this excercise, we define the binary HMM task described above. \n",
        "\n",
        "** Suggestions **\n",
        "* The code has been wrapped into an object. We provide a lazy policy function `def policy_lazy(self, belief, loc)` in which you always stay on one side. \n",
        "* You must code a policy based on threshold in `def policy_threshold(self, threshold, belief, loc)`. The policy takes three inputs: your belief that the fish is on the right (a real number between 0 and 1), your location (\"Left\" or \"Right\"), and an action threshold. When your belief that you are on the same side as the fish drops below the threshold, you choose to switch, and otherwise stay.\n",
        "* For convenience, your belief at time *t* is actually a 2-dimensional vector, with the first element being the belief that the fish are on the left, and the second element is the belief the fish are on the right. You will need to return the action for time *t*, which takes the value of \"stay\" or \"switch\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIxSv_uZ3k7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class binaryHMM():\n",
        "    def __init__(self, params, choose_policy = \"threshold\"):\n",
        "        self.params = params\n",
        "        self.choose_policy = choose_policy\n",
        "    \n",
        "    def generateProcess(self):  \n",
        "        \"\"\"\n",
        "        This function generates the dynamics\n",
        "        \"\"\"\n",
        "        \n",
        "        T, p_stay, rp_high, rp_low, threshold = self.params\n",
        "        world_state = np.zeros((2, T), int)  # 1: high-rewarding box; 0: low-rewarding box\n",
        "        loc = np.zeros(T, int)               # 0: left box, 1: right box\n",
        "        obs = np.zeros(T, int)               # 0: no food, 1: get food\n",
        "        act = np.empty(T, dtype='object')    # \"stay\", or \"switch\" \n",
        "        belief = np.zeros((2, T), float)     # the probability that a certain box is in high-rewarding state,\n",
        "                                             # the beliefs on the two boxes sum up to be 1\n",
        "\n",
        "\n",
        "        rp = np.array([rp_low, rp_high])\n",
        "\n",
        "\n",
        "        for t in range(T):\n",
        "            if t == 0:\n",
        "                world_state[0, t] = 1    # high-rewarding box\n",
        "                world_state[1, t] = 1 - world_state[0, t]\n",
        "                loc[t] = 0\n",
        "                obs[t] = 0\n",
        "                belief_0 = np.random.random(1)[0]\n",
        "                belief[:, t] = np.array([belief_0, 1 - belief_0])\n",
        "\n",
        "                act[t] = self.policy(threshold, belief[:, t], loc[t])\n",
        "\n",
        "            else:\n",
        "                world_state[0, t] = self.world_state_telegraph(world_state[0, t-1], p_stay)\n",
        "                world_state[1, t] = 1 - world_state[0, t]\n",
        "\n",
        "                if act[t - 1] == \"stay\":\n",
        "                    loc[t] = loc[t - 1]\n",
        "                else:  \n",
        "                    loc[t] = 1 - loc[t - 1]\n",
        "\n",
        "                # new observation\n",
        "                obs[t] = np.random.binomial(1, rp[world_state[loc[t], t-1]])\n",
        "                belief[0, t] = self.belief_update(belief[0, t - 1] , loc[t], obs[t], p_stay)\n",
        "                belief[1, t] = 1 - belief[0, t]\n",
        "\n",
        "                act[t] = self.policy(threshold, belief[:, t], loc[t])\n",
        "\n",
        "        return belief, loc, act, obs, world_state\n",
        "\n",
        "\n",
        "    def world_state_telegraph(self, state_past, p_stay):\n",
        "        \"\"\"\n",
        "        state_past is the state of the left site\n",
        "        \"\"\"\n",
        "        ###############################################################################\n",
        "        ## Insert your code here to:\n",
        "        ##        generate the new state for the left site with staying probability p_stay\n",
        "        ###############################################################################\n",
        "        p = np.array([1 - p_stay, p_stay])  # transition probability to good state\n",
        "        state_new = np.random.binomial(1, p[state_past])\n",
        "        return state_new\n",
        "    \n",
        "    def policy(self, threshold, belief, loc):\n",
        "        \"\"\"\n",
        "        This function chooses policy. \n",
        "        \"\"\"\n",
        "        if self.choose_policy == \"threshold\":\n",
        "            act = self.policy_threshold(threshold, belief, loc)\n",
        "        if self.choose_policy == \"lazy\":\n",
        "            act = self.policy_lazy(belief, loc)\n",
        "        \n",
        "        return act\n",
        "    \n",
        "    def policy_threshold(self, threshold, belief, loc):\n",
        "        \"\"\"\n",
        "        This function is the policy based on threshold\n",
        "        \"\"\"\n",
        "        ###############################################################################\n",
        "        ## Insert your code here to:\n",
        "        ##        generate the action based on the current belief and location\n",
        "        ## The belief is a 2d vector with the first element being the belief for the \n",
        "        ## left site, and second element being the belief for the right site. \n",
        "        ## The value of action is either \"stay\" or \"switch\"\n",
        "        ###############################################################################\n",
        "\n",
        "        if belief[loc]  <= threshold:\n",
        "            act = \"switch\"\n",
        "        else:\n",
        "            act = \"stay\"\n",
        "\n",
        "        return act\n",
        "    \n",
        "    def policy_lazy(self, belief, loc):\n",
        "        \"\"\"\n",
        "        This function is a policy where stay is also taken\n",
        "        \"\"\"\n",
        "        act = \"stay\"\n",
        "\n",
        "        return act\n",
        "    \n",
        "    def belief_update(self, belief_past, loc, obs, p_stay):\n",
        "        \"\"\"\n",
        "        using PAST belief on the LEFT box, CURRENT location and observation to update belief\n",
        "        \"\"\"\n",
        "        rp_mat = np.array([[1 - rp_high, rp_high], [1 - rp_low, rp_low]])\n",
        "        \n",
        "        # update belief posterior, p(s[t] | obs(0-t), act(0-t-1))\n",
        "        belief_0 = (belief_past * p_stay  + (1 - belief_past) * (1 - p_stay)) * rp_mat[loc, obs]\n",
        "        belief_1 = ((1 - belief_past) * p_stay + belief_past * (1 - p_stay)) * rp_mat[1-loc, obs]\n",
        "\n",
        "        belief_0 = belief_0 / (belief_0 + belief_1)\n",
        "\n",
        "        return belief_0   \n",
        "\n",
        "    \n",
        "    def plot_dynamics(self, belief, loc, act, obs, world_state):\n",
        "        \"Plot the dynamics of 100 time points\"\n",
        "        T = len(obs)\n",
        "\n",
        "        showlen = min(T, 300)\n",
        "        startT = 0\n",
        "\n",
        "        endT = startT + showlen\n",
        "        showT = range(startT, endT)\n",
        "        time_range = np.linspace(0, showlen - 1)\n",
        "\n",
        "        #fig, [ax0, ax1, ax_loc, ax2, ax3] = plt.subplots(5, 1, figsize=(12, 8))\n",
        "        fig, [ax0, ax1, ax_loc] = plt.subplots(3, 1, figsize=(15, 6))\n",
        "        \n",
        "        ax0.plot(world_state[0, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
        "        ax0.set_ylabel('Fish state', rotation=360, fontsize=18)\n",
        "        ax0.yaxis.set_label_coords(-0.1, 0.25)\n",
        "        ax0.set_xticks([0, showlen, showlen])\n",
        "        ax0.tick_params(axis='both', which='major', labelsize=18)\n",
        "        ax0.set_xlim([0, showlen])\n",
        "        ax0.set_ylim([0, 1.1])\n",
        "        ax0.set_yticks([0, 1])\n",
        "        ax0.tick_params(axis='both', which='major', labelsize=18)\n",
        "        labels = [item.get_text() for item in ax0.get_yticklabels()]\n",
        "        labels[0] = 'Right'\n",
        "        labels[-1] = 'Left'\n",
        "        ax0.set_yticklabels(labels)\n",
        "\n",
        "        ax1.plot(belief[0, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
        "        ax1.plot(time_range, threshold * np.ones(time_range.shape), 'r--')\n",
        "        ax1.plot(time_range, (1 - threshold) * np.ones(time_range.shape), 'c--')\n",
        "        ax1.yaxis.set_label_coords(-0.1, 0.25)\n",
        "        ax1.set_xlabel('time', rotation=360, fontsize=18)\n",
        "        ax1.set_ylabel('Belief on \\n left', rotation=360, fontsize=18)\n",
        "        ax1.tick_params(axis='both', which='major', labelsize=18)\n",
        "        ax1.set_xlim([0, showlen])\n",
        "        ax1.set_yticks([0, 1])\n",
        "        ax1.set_ylim([0, 1.1])\n",
        "        ax1.set_xticks([0, showlen, showlen])\n",
        "        \n",
        "        ax2 = ax1.twinx()\n",
        "        \n",
        "        ax2.plot(time_range, threshold * np.ones(time_range.shape), 'r--')\n",
        "        ax2.plot(time_range, (1 - threshold) * np.ones(time_range.shape), 'c--')  \n",
        "        ax2.set_yticks([threshold, 1 - threshold])\n",
        "        ax2.set_ylim([0, 1.1])\n",
        "        ax2.tick_params(axis='both', which='major', labelsize=18)\n",
        "        labels = [item.get_text() for item in ax2.get_yticklabels()]\n",
        "        labels[0] = 'threshold to switch \\n from left to right'\n",
        "        labels[-1] = 'threshold to switch \\n from right to left'\n",
        "        ax2.set_yticklabels(labels)\n",
        "        \n",
        "        \n",
        "        act_int = (act == \"switch\" ).astype(int)\n",
        "        ax_loc.plot(1 - loc[showT], 'g.-', markersize=12, linewidth=5)\n",
        "        ax_loc.set_ylabel('Your state', rotation=360, fontsize=18)\n",
        "        ax_loc.plot((act_int[showT] - .1) * .8, 'v', markersize=10, label = 'action')\n",
        "        ax_loc.plot(obs[showT] * .5, '*', markersize=5, label = 'reward')\n",
        "        ax_loc.legend(loc=\"upper right\", fontsize = 12)\n",
        "        ax_loc.set_xlim([0, showlen])\n",
        "        ax_loc.set_ylim([0, 1])\n",
        "        ax_loc.set_xticks([0, showlen, showlen])\n",
        "        ax_loc.tick_params(axis='both', which='major', labelsize=18)\n",
        "        labels = [item.get_text() for item in ax_loc.get_yticklabels()]\n",
        "        labels[0] = 'Right'\n",
        "        labels[-1] = 'Left'\n",
        "        ax_loc.set_yticklabels(labels)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "    \n",
        "    def belief_histogram(self, belief, bins = 100):\n",
        "        plt.hist(belief, bins)\n",
        "        plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSakPQ763k8D",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2: Use the binary HMM module to plot dynamics and belief distribution\n",
        "\n",
        "In Excercise 1, we have defined a class for the binary HMM tasl. Now in this excercise, we used the module to generate the dynamics. With the generated data, we will see what the dynamic looks like, and how the beliefs are distributed, etc.\n",
        "\n",
        "** Suggestions **\n",
        "* With the class defined above, create an object of `binaryHMM` given parameters of the dynamic *params* and a parameter for policy. \n",
        "* The parameter for policy can be either *\"lazy\"* or *\"threshold\"*. In the following example the policy is the lazy policy. After you compelete the code for the policy based on threshold in Excercise 1, comment the line using the \"lazy\" policy, and uncomment the line with \"threshold\" policy\".\n",
        "* We have provided an example of the parameters. You can play with the parameters to see the dynamics.\n",
        "* Plot the histgram of the belief states to check the distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM10G8yw3k8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "T = 10000\n",
        "\n",
        "p_stay = .9      # probability fish stay\n",
        "rp_high = .1    # p(catch fish) when you're on their side\n",
        "rp_low = .4     # p(catch fish) when you're on other side\n",
        "\n",
        "threshold = .2    # threshold of belief below which switching is taken\n",
        "\n",
        "params = [T, p_stay, rp_high, rp_low, threshold]\n",
        "\n",
        "binaryHMM_test = binaryHMM(params, \"lazy\")\n",
        "#binaryHMM_test = binaryHMM(params, \"threshold\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn7LY7ab3k8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Plot the dynamics of the binaryHMM task\n",
        "\"\"\"\n",
        "belief, loc, act, obs, world_state = binaryHMM_test.generateProcess()\n",
        "binaryHMM_test.plot_dynamics(belief, loc, act, obs, world_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEk-jNr_3k8K",
        "colab_type": "code",
        "outputId": "94af73fd-76d6-47f8-ba1e-d7da92e04ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "\"\"\"\n",
        "Check the distribution of beliefs.\n",
        "\"\"\"\n",
        "binaryHMM_test.belief_histogram(belief[0, :]) # histgram of the belief on the left box"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0xB3OaT3k8M",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3: Value function \n",
        "Now we have generated behavioral for a parameterized policy. Let's calculate the value of that policy. \n",
        "\n",
        "Specifically, here the value is total expected utility per unit time.\n",
        "\n",
        "$$V(\\theta) = \\frac{1}{T}\\left(\\sum_t U_s(s_t) + U_a(a_t)\\right)$$ \n",
        "\n",
        "where $U_s(s_t)$ is the instataneous utility from the site, and $U_a(a_t)$ is the negative cost for switching.\n",
        "\n",
        "**Suggestions**\n",
        "* Fill in the function `value_function(obs, act, cost_sw)` given a sequence of observations, actions, and the cost of switching. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ67J0kC3k8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_function(obs, act, cost_sw):\n",
        "    \"\"\"\n",
        "    value function \n",
        "    \"\"\"\n",
        "    act_int = (act == \"switch\" ).astype(int)                  # convert labels to binary\n",
        "    T = len(obs)   \n",
        "\n",
        "    ###############################################################################\n",
        "    ## Insert your code here to:\n",
        "    ##        compute the value function\n",
        "    ###############################################################################\n",
        "\n",
        "    value = (np.sum(obs) - np.sum(act_int) * cost_sw) / T     # rate of catching fish - costs\n",
        "\n",
        "    return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwHHQ1Nu3k8P",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 4: Utility function vs threshold \n",
        "Let's find the threshold that yields the highest total value. We have provided code for plotting value versus threshold. You can then find the threshold $\\theta^*$ with the highest value, i.e. the policy that gives optimal control.\n",
        "\n",
        "After that, change the other parameters and explore the resultant values. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_oKA-PY3k8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Brute force for optimal policy, what is the best threshold\n",
        "# This function will be needed for the second excercise \n",
        "def value_threshold(params, cost_sw, step):\n",
        "    threshold_array = np.arange(0, 1 + step, step)\n",
        "    value_array = np.zeros(threshold_array.shape)\n",
        "    \n",
        "    T, p_stay, rp_high, rp_low, _ = params\n",
        "    \n",
        "    for i in range(len(threshold_array)):\n",
        "        threshold = threshold_array[i]\n",
        "        \n",
        "        params = [T, p_stay, rp_high, rp_low, threshold]\n",
        "        binaryHMM_test = binaryHMM(params)\n",
        "        _, _, act, obs, _ = binaryHMM_test.generateProcess()\n",
        "        \n",
        "        value_array[i] = value_function(obs, act, cost_sw)\n",
        "    \n",
        "    return threshold_array, value_array\n",
        "\n",
        "def plot_value_threshold(params, cost_sw, step = 0.05):\n",
        "    threshold_array, value_array = value_threshold(params, cost_sw, step)\n",
        "\n",
        "    fig_, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "    ax.plot(threshold_array, value_array)\n",
        "    ax.set_ylim([np.min(value_array), np.max(value_array)])\n",
        "    ax.set_title('threshold vs value with switching cost c = %.2f'%cost_sw, fontsize = 20)\n",
        "    ax.set_xlabel('threshold', fontsize = 16)\n",
        "    ax.set_ylabel('value', fontsize = 16)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogYtf9Z83k8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# T = 10000\n",
        "# p_stay = .95          # state transiton probability\n",
        "# rp_high = .7\n",
        "# rp_low = .3 #.2\n",
        "\n",
        "cost_sw = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_jKH5ccQ3k8U",
        "colab_type": "code",
        "outputId": "839d29af-3140-4363-979b-fd83210e8ccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "params = [T, p_stay, rp_high, rp_low, _]\n",
        "plot_value_threshold(params, cost_sw)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-w2Onf33k8W",
        "colab_type": "text"
      },
      "source": [
        "when the agent is at different site, the observation and the evidence is different. \n",
        "\n",
        "stronger evidence gives more discrete beliefs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvKYJdzA3k8X",
        "colab_type": "text"
      },
      "source": [
        "1. write the function of the policy \n",
        "f(obs, loc, par) = act\n",
        "\n",
        "2, default dynamic, just wait\n",
        "plug in the policy the function \n",
        "can have a lazy function f(obs, loc) = act_stay, then the students need to define the new policy function, and use that to generate the dynamic\n",
        "\n",
        "use \"stay\" or \"switch\" for the two actions, string type\n",
        "\n",
        "3. evaluate the values \n",
        "\n",
        "4. for loop goes over the thresholds, compute the value, give a funtion to plot the values vs thresholds\n",
        "\n",
        "5. change the cost, plot value vs threshold curves\n",
        "\n",
        "6. histogram of beliefs, explain the plateau\n",
        "\n",
        "7. change the probability trainsiton, reward probabilty, switching cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xUFSyR09nSL",
        "colab_type": "text"
      },
      "source": [
        "Marginal value theorem:\n",
        "\n",
        "Marginal value = b*q1 + (1-b)*q2\n",
        "\n",
        "Other value = (1-b)*q1 + b*q2 - c"
      ]
    }
  ]
}